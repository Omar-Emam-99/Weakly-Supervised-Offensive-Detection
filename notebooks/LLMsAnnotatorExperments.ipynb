{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea6ad318-bdf7-4ef7-902d-1f3314b3a0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tweets': ['@USER She should ask a few native Americans what their take on this is.',\n",
       "  '@USER @USER Go home youâ€™re drunk!!! @USER #MAGA #Trump2020 ðŸ‘ŠðŸ‡ºðŸ‡¸ðŸ‘Š URL',\n",
       "  'Amazon is investigating Chinese employees who are selling internal data to third-party sellers looking for an edge in the competitive marketplace. URL #Amazon #MAGA #KAG #CHINA #TCOT',\n",
       "  '@USER Someone should\\'veTaken\" this piece of shit to a volcano. ðŸ˜‚\"',\n",
       "  '@USER @USER Obama wanted liberals &amp; illegals to move into red states',\n",
       "  '@USER Liberals are all Kookoo !!!',\n",
       "  '@USER @USER Oh noes! Tough shit.',\n",
       "  '@USER was literally just talking about this lol all mass shootings like that have been set ups. itâ€™s propaganda used to divide us on major issues like gun control and terrorism',\n",
       "  '@USER Buy more icecream!!!',\n",
       "  '@USER Canada doesnâ€™t need another CUCK! We already have enough #LooneyLeft #Liberals f**king up our great country! #Qproofs #TrudeauMustGo'],\n",
       " 'labels': [1, 1, 0, 1, 0, 1, 1, 1, 0, 1],\n",
       " 'labels_name': ['OFF',\n",
       "  'OFF',\n",
       "  'NOT',\n",
       "  'OFF',\n",
       "  'NOT',\n",
       "  'OFF',\n",
       "  'OFF',\n",
       "  'OFF',\n",
       "  'NOT',\n",
       "  'OFF']}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "import os\n",
    "from pathlib import Path\n",
    "src_dir = Path.cwd().parent\n",
    "sys.path.append(str(src_dir))\n",
    "from datasets import load_dataset\n",
    "from utilits import *\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = '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'\n",
    "os.environ[\"COHERE_API_TOKEN\"] = \"0kWUANAdjScIFcuHwKGqzv5xY0NTIZhJpwzScJWU\"\n",
    "data_path = \"../dataset\"\n",
    "\n",
    "olid_data = prepare_data(data_path)\n",
    "olid_data[\"train\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac6a249d-4a49-4dea-a7a5-5226ccbae5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Offensive\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "#1.10.8\n",
    "from langchain import PromptTemplate , HuggingFaceHub , LLMChain , FewShotPromptTemplate\n",
    "from langchain.llms import Cohere\n",
    "\n",
    "templete = \"\"\"\n",
    "You are a expert to classify this sentence as offensive or not offensive:\n",
    "\n",
    "Sentence: {sentence}\n",
    "label: <label: offensive, not offensive>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(template=templete, input_variables=[\"sentence\"])\n",
    "\n",
    "llm = HuggingFaceHub(repo_id = \"google/flan-t5-large\")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "#co = cohere.Client('0kWUANAdjScIFcuHwKGqzv5xY0NTIZhJpwzScJWU')\n",
    "cohere_model = Cohere(model=\"command-xlarge\",\n",
    "                      cohere_api_key=\"0kWUANAdjScIFcuHwKGqzv5xY0NTIZhJpwzScJWU\",\n",
    "                      temperature=0)\n",
    "\n",
    "llm_chain_co = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=cohere_model\n",
    ")\n",
    "print(llm_chain_co.run(olid_data[\"train\"][9][\"tweets\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7742e06c-4d0b-4c89-9cdd-6478f4ff75a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "olid_data[\"train\"][9][\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5647f4a5-2bf8-4794-b5f2-c11be9a484a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## FewShot approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ddc5227d-52e3-40d6-851e-c4f2dd738a13",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport cohere\\nfrom langchain.llms import Cohere\\n\\nfrom langchain import FewShotPromptTemplate\\n\\n# create our examples\\nexamples = [\\n    {\\n        \"sentence\": \"Someone should\\'ve Taken this piece of shit to a volcano. ðŸ˜‚\" ,\\n        \"label\" : \"offensive\"\\n    }, {\\n        \"sentence\": \"@USER She should ask a few native Americans what their take on this is.\",\\n        \"label\": \"offensive\"\\n    }, {\\n        \"sentence\": \"@USER @USER Go home youâ€™re drunk!!! @USER #MAGA #Trump2020 ðŸ‘ŠðŸ‡ºðŸ‡¸ðŸ‘Š URL\",\\n        \"label\": \"offensive\"\\n    }, {\\n        \"sentence\": \"Buy more icecream!!!\",\\n        \"label\": \"not offensive\"\\n    }, {\\n        \"sentence\": \"Amazon is investigating Chinese employees who are selling internal data to third-party sellers looking for an edge in the competitive marketplace. URL #Amazon #MAGA #KAG #CHINA #TCOT\",\\n        \"label\": \"not offensive\"\\n    }, {\\n        \"sentence\": \"@USER @USER Obama wanted liberals &amp; illegals to move into red states\",\\n        \"label\": \"not offensive\"\\n    }, {\\n        \"sentence\": \"@USER Besides Jaxâ€™s mom and maybe Ope he is hands down my favorite heâ€™s like the only good person on the show ðŸ˜‚\",\\n        \"label\": \"not offensive\"\\n    }, {\\n        \"sentence\": \"@USER Canada doesnâ€™t need another CUCK! We already have enough #LooneyLeft #Liberals f**king up our great country! #Qproofs #TrudeauMustGo\",\\n        \"label\": \"offensive\"\\n    }\\n]\\n\\n# create a example template\\nexample_template = \"\"\"\\ntweet: {sentence}\\nclassification: {label}\\n\"\"\"\\n\\n# create a prompt example from above template\\nexample_prompt = PromptTemplate(\\n    input_variables=[\"sentence\", \"label\"],\\n    template=example_template\\n)\\n\\n# now break our previous prompt into a prefix and suffix\\n# the prefix is our instructions\\nprefix = \"\"\"The following are examples of sentences that classified as offensive and not offensive \\nHere are some examples:\\n\"\"\"\\n# and the suffix our user input and output indicator\\nsuffix = \"\"\"\\ntweet: {sentence}\\nclassification: \"\"\"\\n\\n# now create the few shot prompt template\\nfew_shot_prompt_template = FewShotPromptTemplate(\\n    examples=examples,\\n    example_prompt=example_prompt,\\n    prefix=prefix,\\n    suffix=suffix,\\n    input_variables=[\"sentence\"],\\n    example_separator=\"\\n\\n\"\\n)\\n\\n#query = \"What is the meaning of life?\"\\n#print(few_shot_prompt_template.format(sentence=query))\\n\\n#co = cohere.Client(\\'0kWUANAdjScIFcuHwKGqzv5xY0NTIZhJpwzScJWU\\')\\ncohere_model = Cohere(model=\"command-xlarge\",\\n                      cohere_api_key=\"0kWUANAdjScIFcuHwKGqzv5xY0NTIZhJpwzScJWU\",\\n                      temperature=0)\\nllm_chain_co = LLMChain(\\n    prompt=few_shot_prompt_template,\\n    llm=cohere_model\\n)\\n\\nprint(llm_chain_co.run(olid_data[\"train\"][205][\"tweets\"]))\\n'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import cohere\n",
    "from langchain.llms import Cohere\n",
    "\n",
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"sentence\": \"Someone should've Taken this piece of shit to a volcano. ðŸ˜‚\" ,\n",
    "        \"label\" : \"offensive\"\n",
    "    }, {\n",
    "        \"sentence\": \"@USER She should ask a few native Americans what their take on this is.\",\n",
    "        \"label\": \"offensive\"\n",
    "    }, {\n",
    "        \"sentence\": \"@USER @USER Go home youâ€™re drunk!!! @USER #MAGA #Trump2020 ðŸ‘ŠðŸ‡ºðŸ‡¸ðŸ‘Š URL\",\n",
    "        \"label\": \"offensive\"\n",
    "    }, {\n",
    "        \"sentence\": \"Buy more icecream!!!\",\n",
    "        \"label\": \"not offensive\"\n",
    "    }, {\n",
    "        \"sentence\": \"Amazon is investigating Chinese employees who are selling internal data to third-party sellers looking for an edge in the competitive marketplace. URL #Amazon #MAGA #KAG #CHINA #TCOT\",\n",
    "        \"label\": \"not offensive\"\n",
    "    }, {\n",
    "        \"sentence\": \"@USER @USER Obama wanted liberals &amp; illegals to move into red states\",\n",
    "        \"label\": \"not offensive\"\n",
    "    }, {\n",
    "        \"sentence\": \"@USER Besides Jaxâ€™s mom and maybe Ope he is hands down my favorite heâ€™s like the only good person on the show ðŸ˜‚\",\n",
    "        \"label\": \"not offensive\"\n",
    "    }, {\n",
    "        \"sentence\": \"@USER Canada doesnâ€™t need another CUCK! We already have enough #LooneyLeft #Liberals f**king up our great country! #Qproofs #TrudeauMustGo\",\n",
    "        \"label\": \"offensive\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# create a example template\n",
    "example_template = \"\"\"\n",
    "tweet: {sentence}\n",
    "classification: {label}\n",
    "\"\"\"\n",
    "\n",
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"sentence\", \"label\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"The following are examples of sentences that classified as offensive and not offensive \n",
    "Here are some examples:\n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "tweet: {sentence}\n",
    "classification: \"\"\"\n",
    "\n",
    "# now create the few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"sentence\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "#query = \"What is the meaning of life?\"\n",
    "#print(few_shot_prompt_template.format(sentence=query))\n",
    "\n",
    "#co = cohere.Client('0kWUANAdjScIFcuHwKGqzv5xY0NTIZhJpwzScJWU')\n",
    "cohere_model = Cohere(model=\"command-xlarge\",\n",
    "                      cohere_api_key=\"0kWUANAdjScIFcuHwKGqzv5xY0NTIZhJpwzScJWU\",\n",
    "                      temperature=0)\n",
    "llm_chain_co = LLMChain(\n",
    "    prompt=few_shot_prompt_template,\n",
    "    llm=cohere_model\n",
    ")\n",
    "\n",
    "print(llm_chain_co.run(olid_data[\"train\"][205][\"tweets\"]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "439aae07-be7a-4a1e-af31-af2f1ef44701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee9f22cf7634c098ceb0bcc6159dcf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "\n",
    "model_id = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "'''\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "'''\n",
    "\n",
    "# begin initializing HF items, need auth token for these\n",
    "hf_auth = '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'\n",
    "model_config = transformers.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "341e3a98-7c10-4acb-b847-b7f86de2c9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers==0.13.3 in /usr/local/lib/python3.9/dist-packages (0.13.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers==0.13.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdd77fb2-9120-4e80-a176-6108d7fd3bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed3d36cb-4233-4229-864f-f979c8b212b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    temperature=0.0,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  # mex number of tokens to generate in the output\n",
    "    repetition_penalty=1.1,  # without this output begins repeating\n",
    "    torch_dtype = torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ecae4ad1-509f-4ef9-84d9-7f2008f566ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@USER Someone should'veTaken\" this piece of shit to a volcano. ðŸ˜‚\"\n",
      "\n",
      "Classify sentence as not-offinsive or offensive , \"offensive\" is that sentence contain toxic or rude or abusive words and meaning , for example:\n",
      "\n",
      "sentence : Someone should'veTaken\" this piece of shit to a volcano\n",
      "label: \"offensive\"\n",
      "\n",
      "\n",
      "sentance : @USER Someone should'veTaken\" this piece of shit to a volcano. ðŸ˜‚\" \n",
      "label:\n",
      "\n",
      "\n",
      "Please note that the above examples are just illustrations, and it is not appropriate to use such language in any context. It is important to be respectful and considerate when communicating with others, even if you are frustrated or upset.\n"
     ]
    }
   ],
   "source": [
    "sentence = olid_data[\"train\"][3][\"tweets\"]\n",
    "print(sentence)\n",
    "templete = f\"\"\"\n",
    "Classify sentence as not-offinsive or offensive , \"offensive\" is that sentence contain toxic or rude or abusive words and meaning , for example:\n",
    "\n",
    "sentence : Someone should'veTaken\" this piece of shit to a volcano\n",
    "label: \"offensive\"\n",
    "\n",
    "\n",
    "sentance : {sentence} \n",
    "label:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "res = generate_text(templete)\n",
    "\n",
    "print(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0f2f145d-3cde-46e6-9ecc-8bca433cc3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import langchain\n",
    "#1.10.8\n",
    "from langchain import PromptTemplate , HuggingFaceHub , LLMChain , FewShotPromptTemplate\n",
    "from langchain.llms import Cohere\n",
    "\n",
    "class LLMAnnotator :\n",
    "    def __init__(self ,\n",
    "                 api_key,\n",
    "                 model_ckpt : str = \"command-xlarge\",\n",
    "                 temperature: float = 0.0):\n",
    "        \n",
    "        self.LLM = Cohere(model=\"command-xlarge\",\n",
    "                      cohere_api_key= api_key,\n",
    "                      temperature=temperature)\n",
    "        self.prompt = \"\"\"\n",
    "        You are a expert to classify this sentence as \"offensive\" or \"not offensive\":\n",
    "\n",
    "        Sentence: {sentence}\n",
    "        label: <label: offensive, not offensive>\n",
    "        \"\"\"\n",
    "        self.prompt_tempelate = PromptTemplate(template=self.prompt, input_variables=[\"sentence\"])\n",
    "        \n",
    "    def annotate(self , data):\n",
    "        llm_cohere =LLMChain(prompt=self.prompt_tempelate,\n",
    "                                                llm=self.LLM)\n",
    "        labels = []\n",
    "        for example in tqdm(data):\n",
    "            gen_label = llm_cohere.run(example)\n",
    "            labels.append(0 if gen_label == \"Not offensive\" else 1)\n",
    "        \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "af3ed1d6-6d14-47af-b96a-54482d6311a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = LLMAnnotator(\"0kWUANAdjScIFcuHwKGqzv5xY0NTIZhJpwzScJWU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ed797232-c29d-41bf-b9f1-3de10f4aba38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b673c735ee344d690375d94e7748bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your text contains a trailing whitespace, which has been trimmed to ensure high quality generations.\n",
      "Your text contains a trailing whitespace, which has been trimmed to ensure high quality generations.\n",
      "Your text contains a trailing whitespace, which has been trimmed to ensure high quality generations.\n",
      "Retrying langchain.llms.cohere.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised CohereAPIError: You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.ai/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions.\n",
      "Retrying langchain.llms.cohere.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised CohereAPIError: You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.ai/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions.\n",
      "Your text contains a trailing whitespace, which has been trimmed to ensure high quality generations.\n",
      "Your text contains a trailing whitespace, which has been trimmed to ensure high quality generations.\n",
      "Your text contains a trailing whitespace, which has been trimmed to ensure high quality generations.\n",
      "Your text contains a trailing whitespace, which has been trimmed to ensure high quality generations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 0, 1]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.annotate(olid_data[\"train\"][\"tweets\"][10:17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ce7ec1-53b4-4b89-87c4-39fb0a3d3603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
